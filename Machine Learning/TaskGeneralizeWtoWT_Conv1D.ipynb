{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gait Video Study \n",
    "### 1D Convolutional neural network (CNN) on task generalization framework 1: train on walking (W) and test on walking while talking (WT) to classify HOA/MS/PD strides and subjects \n",
    "#### Remember to add the original count of frames in a single stride (before down sampling via smoothing) for each stride as an additional artificial feature to add information about speed of the subject to the model\n",
    "\n",
    "1. Save the optimal hyperparameters, confusion matrices and ROC curves for each algorithm.\n",
    "2. Make sure to not use x, y, z, confidence = 0, 0, 0, 0 as points for the model since they are simply missing values and not data points, so make sure to treat them before inputting to model \n",
    "3. Make sure to normalize (z-score) the features before we feed them to the model.\n",
    "4. Make sure to set a random seed wherever required for reproducible results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import imports \n",
    "reload(imports)\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed_value, use_cuda):\n",
    "    '''\n",
    "    To set the random seed for reproducibility of results \n",
    "    Arguments: seed value and use cuda (True if cuda is available)\n",
    "    '''\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    if use_cuda: \n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subjects_common_across_train_test(trial_train, trial_test):\n",
    "    '''\n",
    "    Since we need to implement pure task generalization framework, we must have same subjects across both training and testing trails \n",
    "    Hence, if there are some subjects that are present in the training set but not in the test set or vice versa, we eliminate those \n",
    "    subjects to have only common subjects across training and test sets. \n",
    "    Arguments: data subset for training and testing trial\n",
    "    Returns: PIDs to retain in the training and test subsets with common subjects \n",
    "    '''\n",
    "    \n",
    "    print ('Original number of subjects in training and test sets:', len(trial_train['PID'].unique()), len(trial_test['PID'].unique()))\n",
    "\n",
    "    #Try to use same subjects in trials W and WT for testing on same subjects we train on\n",
    "    print ('Subjects in test set, which are not in training set')\n",
    "    pids_missing_training = [] #PIDs missing in training set (trial W) but are present in the test set (trial WT)\n",
    "    for x in trial_test['PID'].unique():\n",
    "        if x not in trial_train['PID'].unique():\n",
    "            pids_missing_training.append(x)\n",
    "    print (pids_missing_training)\n",
    "    #List of PIDs to retain in the training set \n",
    "    pids_retain_training = [i for i in trial_test['PID'].unique() if i not in pids_missing_training]\n",
    "    \n",
    "    print ('Subjects in training set, which are not in test set')\n",
    "    pids_missing_test = [] #PIDs missing in test set (trial WT) but are present in the training set (trial W)\n",
    "    for x in trial_train['PID'].unique():\n",
    "        if x not in trial_test['PID'].unique():\n",
    "            pids_missing_test.append(x)\n",
    "    print (pids_missing_test)\n",
    "    #List of PIDs to retain in the testing set \n",
    "    pids_retain_test = [i for i in trial_train['PID'].unique() if i not in pids_missing_test]\n",
    "    \n",
    "    print ('Number of subjects in training and test sets after reduction:', len(pids_retain_training), \\\n",
    "           len(pids_retain_test))\n",
    "    #Returning the PIDs to retain in the training and test set\n",
    "    return  pids_retain_training, pids_retain_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch dataset definition\n",
    "class GaitDataset(Dataset):\n",
    "    #We need to add the frame count as an extra feature along with 36 features for each stride \n",
    "    def __init__(self, data_path, labels_csv, pids_retain, framework = 'W', transforms = None, train_data_mean = None, train_data_std = None):   \n",
    "        '''\n",
    "        Arguments: \n",
    "        data_path: data path for downsampled strides \n",
    "        labels_csv: csv file with labels \n",
    "        pids_retain: PIDs to return data for \n",
    "        framework: Task to return data for \n",
    "        transforms: For ToTensor transformation of dataframes\n",
    "        train_data_mean: Mean for the training data (computed beforehand) to z-score normalize the training and testing samples \n",
    "        train_data_std: Standard deviation for the training data (computed beforehand) to z-score normalize the training and testing samples\n",
    "        \n",
    "        Returns:\n",
    "        X: 20 rows for 20 downsampled frames per stride and 37 columns for 37 features for each sample. The features are z-score \n",
    "        normalized and converted to tensor.\n",
    "        y: PID and label for each sample. These values are converted to tensor.\n",
    "        '''\n",
    "        #Assigning the data folder for the downsampled strides \n",
    "        self.data_path = data_path\n",
    "        #Reading the labels file\n",
    "        self.all_labels = pd.read_csv(labels_csv, index_col = 0)\n",
    "        #Retaining only the labels dataframe for framework and PIDs of interest and resetting the index\n",
    "        self.reduced_labels = self.all_labels[self.all_labels.scenario == framework][self.all_labels.PID.isin(pids_retain)].reset_index()\n",
    "        #Setting the labels with index as the key and PID along with to use when computing subject wise evaluation metrics\n",
    "        self.labels = self.reduced_labels[['PID', 'label', 'key']].set_index('key')\n",
    "        self.len = len(self.labels) #Length of the data to use\n",
    "        self.transforms = transforms\n",
    "        self.train_data_mean = train_data_mean\n",
    "        self.train_data_std = train_data_std\n",
    "    \n",
    "    def __len__(self):\n",
    "        #Returns the length of the data \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "#         print ('mean = ', self. train_data_mean)\n",
    "        #Generates one sample of data\n",
    "        #Select key to sample\n",
    "        key = self.reduced_labels['key'].iloc[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = pd.read_csv(data_path+key+'.csv', index_col = 0)\n",
    "        #Creating a new frame count column represting the total original count of frames in a stride \n",
    "        #denoting the speed of the stride\n",
    "        X['frame_count'] = self.reduced_labels[self.reduced_labels['key']==key]['frame_count'].values[0]\n",
    "        y = self.labels.loc[key] #PID and label extracted for the key at the index \n",
    "        #X- 20 rows for 20 downsampled frames per stride and 37 columns for 37 features for each sample\n",
    "        #y - PID and label for each sample\n",
    "        if self.transforms is not None: #Used for loading the data all at once to compute feature-wise mean and std for z-score \n",
    "            X = self.transforms(X.values).squeeze() #20*37 for 20 downsampled frames and 37 features in each frame \n",
    "        else: #Used to load z-score normalized data in batches \n",
    "            X = (X-self.train_data_mean)/self.train_data_std #z-score normalization for all 37 features \n",
    "            X = torch.Tensor(X.values) #converting the dataframe to tensor \n",
    "        y = torch.Tensor(y) #shape = 2 for PID and label \n",
    "#         print (X.shape, y.shape)\n",
    "        return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(data_path, labels_file, pids_retain_train, pids_retain_test, parameter_dict, train_framework, test_framework):\n",
    "    '''\n",
    "    To define the training and testing data loader to load X, y for training and testing sets in batches \n",
    "    Arguments:\n",
    "        data_path: path for downsampled frame csvs\n",
    "        labels_file: path for labels file with key and labels \n",
    "        pids_retain_train: Based on the framework, PIDs to extract training data for \n",
    "        pids_retain_test: Based on the framework, PIDs to extract testing data for \n",
    "        parameter_dict: dictionary with parameters defined \n",
    "        train_framework: task to train on \n",
    "        test_framework: task to test on \n",
    "    Returns: \n",
    "        training_data_loader, testing_data_loader to load X, y for training and testing sets in batches \n",
    "    '''\n",
    "    #Loading the full training data in one go to compute the training data's mean and standard deviation for normalization \n",
    "    #We set the batch_size = len(training_data) for the same \n",
    "    training_data = GaitDataset(data_path, labels_file, pids_retain_train, framework = train_framework, \\\n",
    "                                transforms=transforms.Compose([transforms.ToTensor()]))   \n",
    "    training_data_loader = DataLoader(training_data, batch_size = len(training_data), shuffle = parameter_dict['shuffle'], \\\n",
    "                                      num_workers = parameter_dict['num_workers'])\n",
    "    #Since we loaded all the training data in a single batch, we can read all data and target in one go\n",
    "    data, target = next(iter(training_data_loader))\n",
    "    #Computing the training data mean and standard deviation\n",
    "    training_data_mean = data.reshape(data.shape[0]*data.shape[1], data.shape[2]).mean(axis = 0)\n",
    "    training_data_std = data.reshape(data.shape[0]*data.shape[1], data.shape[2]).std(axis = 0)\n",
    "    print ('Training data mean: ', training_data_mean, training_data_mean.shape)\n",
    "    print ('Training data standard deviation: ', training_data_std, training_data_std.shape)\n",
    "\n",
    "    #With training data mean and standard deviation computed, we can load the z-score normalized training and testing data in batches \n",
    "    training_data = GaitDataset(data_path, labels_file, pids_retain_train, framework = train_framework, \\\n",
    "                                train_data_mean=training_data_mean, train_data_std=training_data_std)   \n",
    "    testing_data = GaitDataset(data_path, labels_file, pids_retain_test, framework = test_framework, \\\n",
    "                              train_data_mean=training_data_mean, train_data_std=training_data_std) \n",
    "\n",
    "    training_data_loader = DataLoader(training_data, batch_size = parameter_dict['batch_size'], shuffle = parameter_dict['shuffle'], \\\n",
    "                                      num_workers = parameter_dict['num_workers'])\n",
    "    testing_data_loader = DataLoader(testing_data, batch_size = parameter_dict['batch_size'], shuffle = parameter_dict['shuffle'], \\\n",
    "                                     num_workers = parameter_dict['num_workers'])\n",
    "    \n",
    "    #To make sure the z-score normalization worked correctly \n",
    "    training_data_loader_check = DataLoader(training_data, batch_size = len(training_data), shuffle = parameter_dict['shuffle'], \\\n",
    "                                      num_workers = parameter_dict['num_workers'])\n",
    "    data_check, target_check= next(iter(training_data_loader_check))\n",
    "    #The normalized means for each of the 37 features must be ~0\n",
    "    print ('Normalized training data\\'s mean:', data_check.reshape(data_check.shape[0]*data_check.shape[1], data_check.shape[2]).mean(axis = 0))\n",
    "    #The normalized standard deviations for each of the 37 feature must be ~1\n",
    "    print ('Normalized training data\\'s standard deviation:', data_check.reshape(data_check.shape[0]*data_check.shape[1], data_check.shape[2]).std(axis = 0))\n",
    "    \n",
    "    return training_data_loader, testing_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, trueY, framework, model_name):\n",
    "    '''\n",
    "    Function to evaluate ML models and plot it's confusion matrix\n",
    "    Input: model, test set, true test set labels, framework name, model name\n",
    "    Computes the stride and subject wise test set evaluation metrics \n",
    "    Returns: Prediction probabilities for HOA/MS/PD and stride and subject wise evaluation metrics \n",
    "    (Accuracy, Precision, Recall, F1 and AUC)\n",
    "    '''\n",
    "    test_labels = trueY['label'] #Dropping the PID\n",
    "#     print ('Test labels', test_labels)\n",
    "    predictions = model.predict(test_features)\n",
    "#     print ('Predictions', predictions)\n",
    "    \n",
    "    #Stride wise metrics \n",
    "    acc = accuracy_score(test_labels, predictions)\n",
    "    #For multiclass predictions, we need to use marco/micro average\n",
    "    p = precision_score(test_labels, predictions, average='macro')  \n",
    "    r = recall_score(test_labels, predictions, average = 'macro')\n",
    "    f1 = f1_score(test_labels, predictions, average= 'macro')\n",
    "    \n",
    "    try:\n",
    "        prediction_prob = model.predict_proba(test_features) #Score of the class with greater label\n",
    "#         print ('Prediction Probability', model.predict_proba(test_features))\n",
    "        \n",
    "    except:\n",
    "        prediction_prob = model.best_estimator_._predict_proba_lr(test_features) #For linear SVM\n",
    "#         print ('Prediction Probability', model.best_estimator_._predict_proba_lr(test_features))\n",
    "    \n",
    "    #For computing the AUC, we would need prediction probabilities for all the 3 classes \n",
    "    auc = roc_auc_score(test_labels, prediction_prob, multi_class = 'ovo', average= 'macro')\n",
    "    print('Stride-based model performance: ', acc, p, r, f1, auc)\n",
    "    \n",
    "    #For computing person wise metrics \n",
    "    temp = copy.deepcopy(trueY) #True label for the stride \n",
    "    temp['pred'] = predictions #Predicted label for the stride \n",
    "    #Saving the stride wise true and predicted labels for calculating the stride wise confusion matrix for each model\n",
    "    temp.to_csv(results_path+ framework + '\\\\stride_wise_predictions_' + str(model_name) + '_' + framework + '.csv')\n",
    "    \n",
    "    x = temp.groupby('PID')['pred'].value_counts().unstack()\n",
    "    #Input for subject wise AUC is probabilities at columns [0, 1, 2]\n",
    "    proportion_strides_correct = x.divide(x.sum(axis = 1), axis = 0).fillna(0) \n",
    "    proportion_strides_correct['True Label'] = trueY.groupby('PID').first()\n",
    "    #Input for precision, recall and F1 score\n",
    "    proportion_strides_correct['Predicted Label'] = proportion_strides_correct[[0, 1, 2]].idxmax(axis = 1) \n",
    "    #Saving the person wise true and predicted labels for calculating the subject wise confusion matrix for each model\n",
    "    proportion_strides_correct.to_csv(results_path+ framework + '\\\\person_wise_predictions_' + \\\n",
    "                                      str(model_name) + '_' + framework + '.csv')\n",
    "    try:\n",
    "        print (model.best_estimator_)\n",
    "    except:\n",
    "        pass\n",
    "    #Person wise metrics \n",
    "    person_acc = accuracy_score(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'])\n",
    "    person_p = precision_score(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'], \\\n",
    "                               average = 'macro')\n",
    "    person_r = recall_score(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'], \\\n",
    "                            average = 'macro')\n",
    "    person_f1 = f1_score(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'], \\\n",
    "                         average = 'macro')\n",
    "    person_auc = roc_auc_score(proportion_strides_correct['True Label'], proportion_strides_correct[[0, 1, 2]], \\\n",
    "                               multi_class = 'ovo', average= 'macro')\n",
    "    print('Person-based model performance: ', person_acc, person_p, person_r, person_f1, person_auc)\n",
    "      \n",
    "    #Plotting and saving the subject wise confusion matrix \n",
    "    plt.figure()\n",
    "    confusion_matrix = pd.crosstab(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'], \\\n",
    "                                   rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\")\n",
    "    plt.savefig(results_path + framework+'\\\\CFmatrix_task_generalize_' + framework + '_'+ ml_model+ '.png', dpi = 350)\n",
    "    plt.show()\n",
    "    return proportion_strides_correct[[0, 1, 2]], [acc, p, r, f1, auc, person_acc, person_p, person_r, person_f1, person_auc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set ROC curves for cohort prediction \n",
    "def plot_ROC(ml_models, testY, predicted_probs_person, framework):\n",
    "    '''\n",
    "    Function to plot the ROC curve for models given in ml_models list \n",
    "    Input: ml_models (name of models to plot the ROC for),  test_Y (true test set labels with PID), \n",
    "        predicted_probs_person (predicted test set probabilities for all 3 classes - HOA/MS/PD), framework (WtoWT / VBWtoVBWT)\n",
    "    Plots and saves the ROC curve with individual class-wise plots and micro/macro average plots \n",
    "    '''\n",
    "    n_classes = 3 #HOA/MS/PD\n",
    "    cohort = ['HOA', 'MS', 'PD']\n",
    "    ml_model_names = {'random_forest': 'RF', 'adaboost': 'Adaboost', 'kernel_svm': 'RBF SVM', 'gbm': 'GBM', \\\n",
    "                      'xgboost': 'Xgboost', 'knn': 'KNN', 'decision_tree': 'DT',  'linear_svm': 'LSVM', \n",
    "                 'logistic_regression': 'LR', 'mlp': 'MLP'}\n",
    "    #PID-wise true labels \n",
    "    person_true_labels = testY.groupby('PID').first()\n",
    "    #Binarizing/getting dummies for the true labels i.e. class 1 is represented as 0, 1, 0\n",
    "    person_true_labels_binarize = pd.get_dummies(person_true_labels.values.reshape(1, -1)[0])  \n",
    "\n",
    "    sns.despine(offset=0)\n",
    "    linestyles = ['-', '-', '-', '-.', '--', '-', '--', '-', '--']\n",
    "    colors = ['b', 'magenta', 'cyan', 'g',  'red', 'violet', 'lime', 'grey', 'pink']\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for idx, ml_model in enumerate(ml_models): #Plotting the ROCs for all models in ml_models list\n",
    "        fig, axes = plt.subplots(1, 1, sharex=True, sharey = True, figsize=(6, 4.5))\n",
    "        axes.plot([0, 1], [0, 1], linestyle='--', label='Majority (AUC = 0.5)', linewidth = 3, color = 'k')\n",
    "        # person-based prediction probabilities for class 0: HOA, 1: MS, 2: PD\n",
    "        model_probs = predicted_probs_person[[ml_model+'_HOA', ml_model+'_MS', ml_model+'_PD']]\n",
    "\n",
    "        for i in range(n_classes): #For 3 classes 0, 1, 2\n",
    "            fpr[i], tpr[i], _ = roc_curve(person_true_labels_binarize.iloc[:, i], model_probs.iloc[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i]) #Computing the AUC score for each class\n",
    "            #Plotting the ROCs for the three classes separately\n",
    "            axes.plot(fpr[i], tpr[i], label = cohort[i] +' ROC (AUC = '+ str(round(roc_auc[i], 3))\n",
    "                +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[i], color = colors[i]) \n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area (AUC)\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(person_true_labels_binarize.values.ravel(), model_probs.values.ravel())\n",
    "        #Micro average AUC of ROC value\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"]) \n",
    "        #Plotting the micro average ROC \n",
    "        axes.plot(fpr[\"micro\"], tpr[\"micro\"], label= 'micro average ROC (AUC = '+ str(round(roc_auc[\"micro\"], 3))\n",
    "                +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[3], color = colors[3])\n",
    "\n",
    "        #Compute the macro-average ROC curve and AUC value\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) # First aggregate all false positive rates\n",
    "        mean_tpr = np.zeros_like(all_fpr) # Then interpolate all ROC curves at this points\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_tpr /= n_classes  # Finally average it and compute AUC\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        #Macro average AUC of ROC value \n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        #Plotting the macro average AUC\n",
    "        axes.plot(fpr[\"macro\"], tpr[\"macro\"], label= 'macro average ROC (AUC = '+ str(round(roc_auc[\"macro\"], 3))\n",
    "            +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[4], color = colors[4])\n",
    "\n",
    "        axes.set_ylabel('True Positive Rate')\n",
    "        axes.set_title('Task generalization '+framework + ' '+ ml_model_names[ml_model])\n",
    "        plt.legend()\n",
    "        # axes[1].legend(loc='upper center', bbox_to_anchor=(1.27, 1), ncol=1)\n",
    "\n",
    "        axes.set_xlabel('False Positive Rate')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_path + framework+'\\\\ROC_task_generalize_' + framework + '_'+ ml_model+ '.png', dpi = 350)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {'path': 'C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\',\n",
    "                  'data_path': 'downsampled_strides\\\\', \n",
    "                  'labels_file': 'labels.csv',\n",
    "                  'results_path': 'C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\DLresults\\\\CNN1D\\\\',\n",
    "                  'batch_size': 100,\n",
    "                  'shuffle':True,\n",
    "                  'num_workers':0 \n",
    "                  #num_workers zero means the loader loads the data inside the main process \n",
    "                  #i.e. training process will work sequentially inside the main process.\n",
    "                 }                \n",
    "use_cuda = torch.cuda.is_available() #use_cuda is True if cuda is available \n",
    "set_random_seed(0, use_cuda) #Setting a fixed random seed for reproducibility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task generalization framework 1: train on walking (W) and test on walking while talking (WT) to classify HOA/MS/PD strides and subjects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of subjects in training and test sets: 32 26\n",
      "Subjects in test set, which are not in training set\n",
      "[403]\n",
      "Subjects in training set, which are not in test set\n",
      "[312, 102, 112, 113, 115, 123, 124]\n",
      "Number of subjects in training and test sets after reduction: 25 25\n",
      "Strides in training set:  1128\n",
      "HOA, MS and PD strides in training set:\n",
      " PD     453\n",
      "MS     341\n",
      "HOA    334\n",
      "Name: cohort, dtype: int64\n",
      "Strides in testing set:  1142\n",
      "HOA, MS and PD strides in testing set:\n",
      " PD     459\n",
      "HOA    351\n",
      "MS     332\n",
      "Name: cohort, dtype: int64\n",
      "Imbalance ratio (controls:MS:PD)= 1:X:Y\n",
      " PD     1.307692\n",
      "HOA    1.000000\n",
      "MS     0.945869\n",
      "Name: cohort, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Task generalization W-> WT framework \n",
    "data_path = parameter_dict['path'] + parameter_dict['data_path']\n",
    "labels_file = parameter_dict['path'] + parameter_dict['labels_file']\n",
    "\n",
    "labels = pd.read_csv(labels_file, index_col = 0)\n",
    "#Trial W for training \n",
    "trialW = labels[labels['scenario']=='W']\n",
    "#Trial WT for testing \n",
    "trialWT = labels[labels['scenario']=='WT']\n",
    "#Returning the PIDs of common subjects in training and testing set\n",
    "pids_retain_trialW, pids_retain_trialWT = list_subjects_common_across_train_test(trialW, trialWT)\n",
    "#Note that both pids_retain_trialW, pids_retain_trialWT will be the same since we are only retaining common subjects \n",
    "#in training and testing trials for a \"pure\" task generalization framework\n",
    "\n",
    "#Showing the statistics and imbalance ratio of training and testing data \n",
    "trialW_reduced = trialW[trialW.PID.isin(pids_retain_trialW)]\n",
    "print ('Strides in training set: ', trialW_reduced.shape[0])\n",
    "print ('HOA, MS and PD strides in training set:\\n', trialW_reduced['cohort'].value_counts())\n",
    "\n",
    "trialWT_reduced = trialWT[trialWT.PID.isin(pids_retain_trialWT)]\n",
    "print ('Strides in testing set: ', trialWT_reduced.shape[0])\n",
    "print ('HOA, MS and PD strides in testing set:\\n', trialWT_reduced['cohort'].value_counts())\n",
    "print ('Imbalance ratio (controls:MS:PD)= 1:X:Y\\n', trialWT_reduced['cohort'].value_counts()/trialWT_reduced['cohort'].value_counts()['HOA'])\n",
    "\n",
    "framework = 'WtoWT' #Defining the task generalization framework of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To define the training and testing data loader to load X, y for training and testing sets in batches \n",
    "training_data_loader, testing_data_loader = get_data_loaders(data_path, labels_file, pids_retain_trialW, \\\n",
    "                                                             pids_retain_trialWT, parameter_dict, 'W', 'WT')\n",
    "print ('No. of batches in training loader', len(training_data_loader))\n",
    "print ('No. of batches in testing loader', len(testing_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=7, out_channels=20, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=20, out_channels=10, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        log_probs = F.log_softmax(x, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch_idx, (data, target) in enumerate(training_data_loader):\n",
    "    print (data)\n",
    "    print (target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.random.uniform(-10, 10, 70).reshape(1, 7, -1)\n",
    "# Y = np.random.randint(0, 9, 10).reshape(1, 1, -1)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "num_classes = 3\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Error messages are much better if on CPU\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, preds) in enumerate(trainDataloader):\n",
    "        #Get data to cuda if possible\n",
    "        data = data.to(device=device).squeeze(1)\n",
    "        #print(\"data nan? \", (torch.isnan(data)).any())\n",
    "        #print(data)\n",
    "        #print(\"original data shape: \",data.shape)\n",
    "        label = preds.to(device=device)\n",
    "        #print(label)\n",
    "        #print(\"original label shape: \",label.shape)\n",
    "\n",
    "        # forward\n",
    "        #print(targets)\n",
    "        pred = model(data.float())\n",
    "        pred = torch.squeeze(pred)\n",
    "        #print(\"post squeeze output shape: \", pred.shape)\n",
    "        loss = criterion(pred, label.float())\n",
    "        \n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    #if loader.dataset.train:\n",
    "    #    print(\"Checking accuracy on training data\")\n",
    "    #else:\n",
    "    #    print(\"Checking accuracy on test data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x.float())\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with \\\n",
    "              accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
    "        )\n",
    "    # Set model back to train\n",
    "    model.train()\n",
    "\n",
    "model = CNN1D().double()\n",
    "print(model(torch.tensor(X)).shape)\n",
    "torch.save(model.state_dict(), results_path+framework+'\\\\'+ model_save_name+ '.pt')\n",
    "# model = cnn_model.ConvNet(32).double()\n",
    "# model.cuda()\n",
    "# model.load_state_dict(torch.load(results_path+framework+'\\\\'+ model_save_name+ '.pt'))\n",
    "\n",
    "#Maybe use skorch for hyperparameter grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = 'CNN1D'\n",
    "metrics = pd.DataFrame(columns = ml_model) #Dataframe to store accuracies for each ML model for raw data \n",
    "\n",
    "#For storing predicted probabilities for person (for all classes HOA/MS/PD) to show ROC curves \n",
    "predicted_probs_person = pd.DataFrame(columns = [ml_model + cohort for cohort in ['_HOA', '_MS', '_PD'] ]) \n",
    "\n",
    "print (ml_model)\n",
    "predict_probs_person, stride_person_metrics = models(trainX_norm, trainY, testX_norm, testY, ml_model, framework)  \n",
    "metrics[ml_model] = stride_person_metrics\n",
    "predicted_probs_person[ml_model+'_HOA'] = predict_probs_person[0]\n",
    "predicted_probs_person[ml_model+'_MS'] = predict_probs_person[1]\n",
    "predicted_probs_person[ml_model+'_PD'] = predict_probs_person[2]\n",
    "print ('********************************')\n",
    "\n",
    "metrics.index = ['stride_accuracy', 'stride_precision', 'stride_recall', 'stride_F1', 'stride_AUC', 'person_accuracy', \n",
    "                     'person_precision', 'person_recall', 'person_F1', 'person_AUC']  \n",
    "metrics.to_csv(results_path+'task_generalize_'+framework+'_result_metrics.csv')\n",
    "predicted_probs_person.to_csv(results_path+'task_generalize_'+framework+'_prediction_probs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC(ml_models, testY, predicted_probs_person, framework)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
