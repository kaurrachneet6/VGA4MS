{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gait Video Study \n",
    "### Traditional ML algorithms on task+subject generalization together frameworks, namely a) train on some subjects in W-> test on separate set of subjects in WT and b) train on some subjects in VBW-> test on separate set of subjects in VBWT to classify HOA/MS/PD strides and subjects \n",
    "#### Remember to add the original count of frames in a single stride (before down sampling via smoothing) for each stride as an additional artificial feature to add information about speed of the subject to the model\n",
    "\n",
    "1. Save the optimal hyperparameters, confusion matrices and ROC curves for each algorithm.\n",
    "2. Make sure to not use x, y, z, confidence = 0, 0, 0, 0 as points for the model since they are simply missing values and not data points, so make sure to treat them before inputting to model \n",
    "3. Make sure to normalize (mean substract) the features before we feed them to the model.\n",
    "4. We use the summary statistics as range, CoV and asymmetry between the right and left limbs as the features to input to the traditional models requiring fixed size 1D input for each training/testing set sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import imports \n",
    "reload(imports)\n",
    "from imports import *\n",
    "from split import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\'\n",
    "data_path = path+'traditional_methods_dataframe.csv'\n",
    "results_path = 'C:\\\\Users\\\\Rachneet Kaur\\\\Box\\Gait Video Project\\\\MLresults\\\\'\n",
    "\n",
    "data = pd.read_csv(data_path, index_col= 0)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, yoriginal_, ypredicted_, framework, model_name):\n",
    "    '''\n",
    "    Arguments: trained model, test set, true and predicted labels for test set, framework and model name \n",
    "    Returns: predicted probabilities and labels for each class, stride and subject based evaluation metrics \n",
    "    Saves the csv files for stride wise predictions and subject wise predictions for confusion matrix \n",
    "    '''\n",
    "    #For creating the stride wise confusion matrix, we append the true and predicted labels for strides in each fold to this \n",
    "    #test_strides_true_predicted_labels dataframe \n",
    "    test_strides_true_predicted_labels = pd.DataFrame()\n",
    "    #For creating the subject wise confusion matrix, we append the true and predicted labels for subjects in each fold to this\n",
    "    #test_subjects_true_predicted_labels dataframe\n",
    "    test_subjects_true_predicted_labels = pd.DataFrame()\n",
    "    \n",
    "    best_index = model.cv_results_['mean_test_accuracy'].argmax()\n",
    "    print('best_params: ', model.cv_results_['params'][best_index])\n",
    "\n",
    "    #Stride-wise metrics \n",
    "    stride_metrics_mean, stride_metrics_std = [], [] #Mean and SD of stride based metrics - Acc, P, R, F1, AUC (in order)\n",
    "    scores={'accuracy': make_scorer(acc), 'precision':make_scorer(precision_score, average = 'macro'), \\\n",
    "            'recall':make_scorer(recall_score, average = 'macro'), 'f1': make_scorer(f1_score, average = 'macro'), \\\n",
    "           'auc': make_scorer(roc_auc_score, average = 'macro', multi_class = 'ovo', needs_proba= True)}\n",
    "    \n",
    "    for score in scores:\n",
    "        stride_metrics_mean.append(model.cv_results_['mean_test_'+score][best_index])\n",
    "        stride_metrics_std.append(model.cv_results_['std_test_'+score][best_index])\n",
    "    print('Stride-based model performance (mean): ', stride_metrics_mean)\n",
    "    print('Stride-based model performance (standard deviation): ', stride_metrics_std)\n",
    "    n_folds = 5\n",
    "    person_acc, person_p, person_r, person_f1, person_auc = [], [], [], [], []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        #For each fold, there are 2 splits: test and train (in order) and we need to retrieve the index \n",
    "        #of only test set for required 5 folds (best index)\n",
    "        temp = test_features.loc[yoriginal_[(best_index*n_folds) + (i)].index] #True labels for the test strides in each fold\n",
    "        temp['pred'] = ypredicted_[(best_index*n_folds) + (i)] #Predicted labels for the strides in the test set in each fold\n",
    "#         print ('temp_pred', temp['pred'])\n",
    "        #Appending the test strides' true and predicted label for each fold to compute stride-wise confusion matrix \n",
    "        test_strides_true_predicted_labels = test_strides_true_predicted_labels.append(temp)\n",
    "        \n",
    "        x = temp.groupby('PID')['pred'].value_counts().unstack()\n",
    "#         print ('x', x)\n",
    "        #Input for subject wise AUC is probabilities at columns [0, 1, 2]\n",
    "        proportion_strides_correct = pd.DataFrame(columns = [0, 1, 2])\n",
    "        probs_stride_wise = x.divide(x.sum(axis = 1), axis = 0).fillna(0)\n",
    "        proportion_strides_correct[probs_stride_wise.columns] = probs_stride_wise\n",
    "        proportion_strides_correct.fillna(0, inplace=True)\n",
    "        proportion_strides_correct['True Label'] = test_features.groupby('PID').first()\n",
    "        #Input for precision, recall and F1 score\n",
    "        proportion_strides_correct['Predicted Label'] = proportion_strides_correct[[0, 1, 2]].idxmax(axis = 1) \n",
    "        #Appending the test subjects' true and predicted label for each fold to compute subject-wise confusion matrix \n",
    "        test_subjects_true_predicted_labels = test_subjects_true_predicted_labels.append(proportion_strides_correct)          \n",
    "            \n",
    "        #Person wise metrics for each fold \n",
    "        person_acc.append(accuracy_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_p.append(precision_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label'], \\\n",
    "                                       average = 'macro'))\n",
    "        person_r.append(recall_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label'], \\\n",
    "                                    average = 'macro'))\n",
    "        person_f1.append(f1_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label'], \\\n",
    "                                  average = 'macro'))\n",
    "        person_auc.append(roc_auc_score(proportion_strides_correct['True Label'], proportion_strides_correct[[0, 1, 2]], \\\n",
    "                                        multi_class = 'ovo', average= 'macro'))\n",
    "\n",
    "    #Mean and standard deviation for person-based metrics \n",
    "    person_means = [np.mean(person_acc), np.mean(person_p), np.mean(person_r), np.mean(person_f1), np.mean(person_auc)]\n",
    "    person_stds = [np.std(person_acc), np.std(person_p), np.std(person_r), np.std(person_f1), np.std(person_auc)]\n",
    "    print('Person-based model performance (mean): ', person_means)\n",
    "    print('Person-based model performance (standard deviation): ', person_stds)\n",
    "    \n",
    "    #Saving the stride and person wise true and predicted labels for calculating the \n",
    "    #stride and subject wise confusion matrix for each model\n",
    "    test_strides_true_predicted_labels.to_csv(results_path+ framework + '\\\\stride_wise_predictions_' + \\\n",
    "                                      str(model_name) + '_' + framework + '.csv')\n",
    "    test_subjects_true_predicted_labels.to_csv(results_path+ framework + '\\\\person_wise_predictions_' + \\\n",
    "                                      str(model_name) + '_' + framework + '.csv')\n",
    "    \n",
    "    return test_subjects_true_predicted_labels, [stride_metrics_mean, stride_metrics_std, person_means, person_stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true,y_pred):\n",
    "    '''\n",
    "    Returns the accuracy \n",
    "    Saves the true and predicted labels for training and test sets\n",
    "    '''\n",
    "    global yoriginal, ypredicted\n",
    "    yoriginal.append(y_true)\n",
    "    ypredicted.append(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do not use LDA/QDA since our features are not normally distributed \n",
    "def models(X, Y, model_name = 'random_forest', framework = 'W'):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X, Y, PID groups so that strides of each person are either in training or in testing set\n",
    "    model: model_name, framework we wish to run the code for\n",
    "    Returns: predicted probabilities and labels for each class, stride and subject based evaluation metrics \n",
    "    '''\n",
    "    Y_ = Y['label'] #Dropping the PID\n",
    "    groups_ = Y['PID']\n",
    "    #We use stratified group K-fold to sample our strides data\n",
    "    gkf = StratifiedGroupKFold(n_splits=5) \n",
    "    scores={'accuracy': make_scorer(acc), 'precision':make_scorer(precision_score, average = 'macro'), \\\n",
    "            'recall':make_scorer(recall_score, average = 'macro'), 'f1': make_scorer(f1_score, average = 'macro'), \\\n",
    "            'auc': make_scorer(roc_auc_score, average = 'macro', multi_class = 'ovo', needs_proba=True)}\n",
    "    if(model_name == 'random_forest'): #Random Forest\n",
    "        grid = {\n",
    "       'randomforestclassifier__n_estimators': [40,45,50],\\\n",
    "       'randomforestclassifier__max_depth' : [15,20,25,None],\\\n",
    "       'randomforestclassifier__class_weight': [None, 'balanced'],\\\n",
    "       'randomforestclassifier__max_features': ['auto','sqrt','log2', None],\\\n",
    "       'randomforestclassifier__min_samples_leaf':[1,2,0.1,0.05]\n",
    "        }\n",
    "        #For z-score scaling on training and use calculated coefficients on test set\n",
    "        rf_grid = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))\n",
    "        grid_search = GridSearchCV(rf_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name == 'adaboost'): #Adaboost\n",
    "        ada_grid = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=0))\n",
    "        grid = {\n",
    "        'adaboostclassifier__n_estimators':[50, 75, 100, 125, 150],\\\n",
    "        'adaboostclassifier__learning_rate':[0.01,.1, 1, 1.5, 2]\\\n",
    "        }\n",
    "        grid_search = GridSearchCV(ada_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "        \n",
    "    if(model_name == 'kernel_svm'): #RBF SVM\n",
    "        svc_grid = make_pipeline(StandardScaler(), SVC(kernel = 'rbf', probability=True, random_state=0))\n",
    "        grid = {\n",
    "        'svc__gamma':[0.0001, 0.001, 0.1, 1, 10, ]\\\n",
    "        }\n",
    "        grid_search = GridSearchCV(svc_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "\n",
    "    if(model_name == 'gbm'): #GBM\n",
    "        gbm_grid = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=0))\n",
    "        grid = {\n",
    "        'gradientboostingclassifier__learning_rate':[0.15,0.1,0.05], \\\n",
    "        'gradientboostingclassifier__n_estimators':[50, 100, 150],\\\n",
    "        'gradientboostingclassifier__max_depth':[2,4,7],\\\n",
    "        'gradientboostingclassifier__min_samples_split':[2,4], \\\n",
    "        'gradientboostingclassifier__min_samples_leaf':[1,3],\\\n",
    "        'gradientboostingclassifier__max_features':['auto','sqrt','log2', None],\\\n",
    "        }\n",
    "        grid_search = GridSearchCV(gbm_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name=='xgboost'): #Xgboost\n",
    "        xgb_grid = make_pipeline(StandardScaler(), xgboost.XGBClassifier(random_state=0))\n",
    "        grid = {\n",
    "            'xgbclassifier__min_child_weight': [1, 5],\\\n",
    "            'xgbclassifier__gamma': [0.1, 0.5, 1, 1.5, 2],\\\n",
    "            'xgbclassifier__subsample': [0.6, 0.8, 1.0],\\\n",
    "            'xgbclassifier__colsample_bytree': [0.6, 0.8, 1.0],\\\n",
    "            'xgbclassifier__max_depth': [5, 7, 8]\n",
    "        }\n",
    "        grid_search = GridSearchCV(xgb_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name == 'knn'): #KNN\n",
    "        knn_grid = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "        grid = {\n",
    "            'kneighborsclassifier__n_neighbors': [1, 3, 4, 5, 10],\\\n",
    "            'kneighborsclassifier__p': [1, 2, 3, 4, 5]\\\n",
    "        }\n",
    "        grid_search = GridSearchCV(knn_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "        \n",
    "    if(model_name == 'decision_tree'): #Decision Tree\n",
    "        dec_grid = make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=0))\n",
    "        #For z-score scaling on training and use calculated coefficients on test set\n",
    "        grid = {'decisiontreeclassifier__min_samples_split': range(2, 50)}\n",
    "        grid_search = GridSearchCV(dec_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "\n",
    "    if(model_name == 'linear_svm'): #Linear SVM\n",
    "        lsvm_grid = make_pipeline(StandardScaler(), SVC(kernel = 'linear', probability=True, random_state=0)) #LinearSVC(random_state=0, probability= True))\n",
    "        grid = {\n",
    "            'svc__gamma':[0.0001, 0.001, 0.1, 1, 10, ]\\\n",
    "\n",
    "        }\n",
    "        grid_search = GridSearchCV(lsvm_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name == 'logistic_regression'): #Logistic regression\n",
    "        lr_grid = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "        grid = {\n",
    "            'logisticregression__random_state': [0]}\n",
    "            \n",
    "        grid_search = GridSearchCV(lr_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name == 'mlp'):\n",
    "        mlp_grid = make_pipeline(StandardScaler(), MLPClassifier(random_state = 0, activation='relu', solver='adam',\\\n",
    "                                                       learning_rate = 'adaptive', learning_rate_init=0.001, \n",
    "                                                        shuffle=False, max_iter = 200))\n",
    "        grid = {\n",
    "            'mlpclassifier__hidden_layer_sizes': [(128, 8, 8, 128, 32), (50, 50, 50, 50, 50, 50, 150, 100, 10), \n",
    "                                  (50, 50, 50, 50, 50, 60, 30, 20, 50), (50, 50, 50, 50, 50, 150, 10, 60, 150),\n",
    "                                  (50, 50, 50, 50, 50, 5, 50, 10, 5), (50, 50, 50, 50, 50, 5, 50, 150, 150),\n",
    "                                  (50, 50, 50, 50, 50, 5, 30, 50, 20), (50, 50, 50, 50, 10, 150, 20, 20, 30),\n",
    "                                  (50, 50, 50, 50, 30, 150, 100, 20, 100), (50, 50, 50, 50, 30, 5, 100, 20, 100),\n",
    "                                  (50, 50, 50, 50, 60, 50, 50, 60, 60), (50, 50, 50, 50, 20, 50, 60, 20, 20),\n",
    "                                  (50, 50, 50, 10, 50, 10, 150, 60, 150), (50, 50, 50, 10, 50, 150, 30, 150, 5),\n",
    "                                  (50, 50, 50, 10, 50, 20, 150, 5, 10), (50, 50, 50, 10, 150, 50, 20, 20, 100), \n",
    "                                  (50, 50, 50, 30, 100, 5, 30, 150, 30), (50, 50, 50, 50, 100, 150, 100, 200), \n",
    "                                  (50, 50, 50, 5, 5, 100, 100, 150), (50, 50, 5, 50, 200, 100, 150, 5), \n",
    "                                  (50, 50, 5, 5, 200, 100, 50, 30), (50, 50, 5, 10, 5, 200, 200, 10), \n",
    "                                  (50, 50, 5, 30, 5, 5, 50, 10), (50, 50, 5, 200, 50, 5, 5, 50), \n",
    "                                  (50, 50,50, 5, 5, 100, 100, 150), (5, 5, 5, 5, 5, 100, 50, 5, 50, 50), \n",
    "                                  (5, 5, 5, 5, 5, 100, 20, 100, 30, 30), (5, 5, 5, 5, 5, 20, 20, 5, 30, 100), \n",
    "                                  (5, 5, 5, 5, 5, 20, 20, 100, 10, 10), (5, 5, 5, 5, 10, 10, 30, 50, 10, 10), \n",
    "                                  (5, 5, 5, 5, 10, 100, 30, 30, 30, 10), (5, 5, 5, 5, 10, 100, 50, 10, 50, 10), \n",
    "                                  (5, 5, 5, 5, 10, 100, 20, 100, 30, 5), (5, 5, 5, 5, 30, 5, 20, 30, 100, 50), \n",
    "                                  (5, 5, 5, 5, 30, 100, 20, 50, 20, 30), (5, 5, 5, 5, 50, 30, 5, 50, 10, 100), \n",
    "                                  (21, 21, 7, 84, 21, 84, 84), (21, 21, 5, 42, 42, 7, 42), (21, 84, 7, 7, 7, 84, 5), \n",
    "                                  (21, 7, 84, 5, 5, 21, 120), (42, 5, 21, 21, 21, 5, 120), (42, 5, 42, 84, 7, 120, 84), \n",
    "                                  (50, 100, 10, 5, 100, 25), (10, 10, 25, 50, 25, 5), (50, 50, 50, 50, 50, 20, 30, 100, 60)]\n",
    "\n",
    "        }\n",
    "        grid_search = GridSearchCV(mlp_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    grid_search.fit(X, Y_, groups=groups_) #Fitting on the training set to find the optimal hyperparameters \n",
    "    test_subjects_true_predicted_labels, stride_person_metrics = evaluate(grid_search, Y, yoriginal, ypredicted, framework, model_name)\n",
    "    return test_subjects_true_predicted_labels, stride_person_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curves \n",
    "def plot_ROC(ml_model, test_set_true_predicted_labels, framework):\n",
    "    '''\n",
    "    Function to plot the ROC curve and confusion matrix for model given in ml_model name \n",
    "    Input: ml_models (name of models to plot the ROC for),  test_Y (true test set labels with PID), \n",
    "        predicted_probs_person (predicted test set probabilities for all 3 classes - HOA/MS/PD), framework (WtoWT / VBWtoVBWT)\n",
    "    Plots and saves the ROC curve with individual class-wise plots and micro/macro average plots \n",
    "    '''\n",
    "    n_classes = 3 #HOA/MS/PD\n",
    "    cohort = ['HOA', 'MS', 'PD']\n",
    "    ml_model_names = {'random_forest': 'RF', 'adaboost': 'AdaBoost', 'kernel_svm': 'RBF SVM', 'gbm': 'GBM', \\\n",
    "                  'xgboost': 'Xgboost', 'knn': 'KNN', 'decision_tree': 'DT',  'linear_svm': 'LSVM', \n",
    "             'logistic_regression': 'LR', 'mlp':'MLP'}\n",
    "\n",
    "    #Binarizing/getting dummies for the true labels i.e. class 1 is represented as 0, 1, 0\n",
    "    test_features_binarize = pd.get_dummies(test_set_true_predicted_labels['True Label'].values)     \n",
    "    sns.despine(offset=0)\n",
    "    linestyles = ['-', '-', '-', '-.', '--', '-', '--', '-', '--']\n",
    "    colors = ['b', 'magenta', 'cyan', 'g',  'red', 'violet', 'lime', 'grey', 'pink']\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, sharex=True, sharey = True, figsize=(6, 4.5))\n",
    "    axes.plot([0, 1], [0, 1], linestyle='--', label='Majority (AUC = 0.5)', linewidth = 3, color = 'k')\n",
    "    # person-based prediction probabilities for class 0: HOA, 1: MS, 2: PD\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    tpr, fpr, roc_auc = dict(), dict(), dict()\n",
    "    for i in range(n_classes): #n_classes = 3\n",
    "        fpr[i], tpr[i], _ = roc_curve(test_features_binarize.iloc[:, i], test_set_true_predicted_labels.loc[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        #Plotting the ROCs for the three classes separately\n",
    "        axes.plot(fpr[i], tpr[i], label = cohort[i] +' ROC (AUC = '+ str(round(roc_auc[i], 3))\n",
    "            +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[i], color = colors[i])\n",
    "        \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_features_binarize.values.ravel(),\\\n",
    "                                              test_set_true_predicted_labels[[0, 1, 2]].values.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    #Plotting the micro average ROC \n",
    "    axes.plot(fpr[\"micro\"], tpr[\"micro\"], label= 'micro average ROC (AUC = '+ str(round(roc_auc[\"micro\"], 3))\n",
    "            +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[3], color = colors[3])\n",
    "    \n",
    "    #Compute the macro-average ROC curve and AUC value\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) # First aggregate all false positive rates\n",
    "    mean_tpr = np.zeros_like(all_fpr) # Then interpolate all ROC curves at this points\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes  # Finally average it and compute AUC\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    #Macro average AUC of ROC value \n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])    \n",
    "    #Plotting the macro average AUC\n",
    "    axes.plot(fpr[\"macro\"], tpr[\"macro\"], label= 'macro average ROC (AUC = '+ str(round(roc_auc[\"macro\"], 3))\n",
    "        +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[4], color = colors[4])\n",
    "    \n",
    "    axes.set_ylabel('True Positive Rate')\n",
    "    axes.set_title('Subject generalization '+framework + ' '+ ml_model_names[ml_model])\n",
    "    plt.legend()\n",
    "    # axes[1].legend(loc='upper center', bbox_to_anchor=(1.27, 1), ncol=1)\n",
    "\n",
    "    axes.set_xlabel('False Positive Rate')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_path + framework+'\\\\ROC_subject_generalize_' + framework + '_'+ ml_model+ '.png', dpi = 350)\n",
    "    plt.show()\n",
    "    \n",
    "    #Plotting and saving the subject wise confusion matrix \n",
    "    plt.figure()\n",
    "    confusion_matrix = pd.crosstab(test_set_true_predicted_labels['True Label'], test_set_true_predicted_labels['Predicted Label'], \\\n",
    "                                   rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\")\n",
    "    plt.savefig(results_path + framework+'\\\\CFmatrix_subject_generalize_' + framework + '_'+ ml_model+ '.png', dpi = 350)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_models(ml_models, X, Y, framework):\n",
    "    '''\n",
    "    Function to run the ML models for the required framework\n",
    "    Arguments: names of ml_models, X, Y, framework \n",
    "    Returns and saves .csv for evaluation metrics and tprs/fprs/rauc for the ROC curves \n",
    "    '''\n",
    "    metrics = pd.DataFrame(columns = ml_models) #Dataframe to store accuracies for each ML model for raw data \n",
    "    for ml_model in ml_models:\n",
    "        print (ml_model)\n",
    "        global yoriginal, ypredicted\n",
    "        yoriginal = []\n",
    "        ypredicted = []\n",
    "        test_subjects_true_predicted_labels, stride_person_metrics = models(X, Y, ml_model, framework)\n",
    "        metrics[ml_model] = sum(stride_person_metrics, [])\n",
    "        plot_ROC(ml_model, test_subjects_true_predicted_labels, framework)\n",
    "        print ('********************************')\n",
    "    metrics.index = ['stride_mean_accuracy', 'stride_mean_precision', 'stride_mean_recall', 'stride_mean_F1', \\\n",
    "                         'stride_mean_AUC', 'stride_std_accuracy', 'stride_std_precision', 'stride_std_recall', 'stride_std_F1', \\\n",
    "                         'stride_std_AUC','person_mean_accuracy', 'person_mean_precision', 'person_mean_recall', 'person_mean_F1',\\\n",
    "                         'person_mean_AUC', 'person_std_accuracy', 'person_std_precision', 'person_std_recall', 'person_std_F1',\\\n",
    "                         'person_std_AUC']  \n",
    "    #Saving the evaluation metrics and tprs/fprs/rauc for the ROC curves \n",
    "    metrics.to_csv(results_path+framework+'\\\\subject_generalize_'+framework+'_result_metrics.csv')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_common_PIDs(data, frameworks = ['W', 'WT', 'SLW', 'SLWT']):\n",
    "    '''\n",
    "    Since we need to compare across sub-frameworks, we must have same subjects across all sub-frameworks \n",
    "    Hence, if there are some subjects that are present in the one sub-frameworks, say W but not in another and vice versa, we eliminate those \n",
    "    subjects to have only common subjects acrossall subframeworks. \n",
    "    Arguments: \n",
    "        data: labels.csv file with all consolidated information,\n",
    "        frameworks: list of frameworks for which common PIDs need to be filtered\n",
    "    Returns: \n",
    "        common_pids: list of common PIDs/subjects across all the frameworks \n",
    "    '''\n",
    "    \n",
    "    original_pids = {} #Dictionary with original number of PIDs in each framework (task)\n",
    "    for framework in frameworks:\n",
    "        #Appending the original PIDs for each task\n",
    "        original_pids[framework] = data[data.scenario==framework].PID.unique()\n",
    "        print ('Original number of subjects in task', framework, 'are:', len(original_pids[framework]))\n",
    "\n",
    "    #List of common PIDs across all frameworks\n",
    "    common_pids = set(original_pids[frameworks[0]])\n",
    "    for framework in frameworks[1:]:\n",
    "        common_pids.intersection_update(original_pids[framework])\n",
    "    common_pids = list(common_pids)\n",
    "    print ('Common number of subjects across all frameworks: ', len(common_pids))\n",
    "    print ('Common subjects across all frameworks: ', common_pids)\n",
    "    return common_pids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task+subject generalization together framework 1: train on walking (W) and test on walking while talking (WT) to classify HOA/MS/PD strides and subjects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial W for the first framework of subject generalization\n",
    "trialW = data[data['scenario']=='W']\n",
    "print ('Original number of subjects in trial W for cross validation:', len(trialW['PID'].unique()))\n",
    "print ('Number of subjects in trial W in each cohort:\\n', trialW.groupby('PID').first()['cohort'].value_counts())\n",
    "\n",
    "cols_to_drop = ['PID', 'key', 'cohort', 'trial', 'scenario', 'video', 'stride_number', 'label']\n",
    "#Shuffling the cross validation stride data\n",
    "trialW = shuffle(trialW, random_state = 0)\n",
    "#CV for people generalize so no train-test split\n",
    "X = trialW.drop(cols_to_drop, axis = 1)\n",
    "Y = trialW[['PID', 'label']]\n",
    "\n",
    "#Total strides and imbalance of labels in the training and testing set\n",
    "#Training set \n",
    "print('Strides in trial W for cross validation: ', len(trialW))\n",
    "print ('HOA, MS and PD strides in trial W:\\n', trialW['cohort'].value_counts())\n",
    "print ('Imbalance ratio in trial W (controls:MS:PD)= 1:X:Y\\n', trialW['cohort'].value_counts()/trialW['cohort'].value_counts()['HOA'])\n",
    "\n",
    "#Defining the framework of interest\n",
    "framework = 'W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial W for training \n",
    "trialW = data[data['scenario']=='W']\n",
    "#Trial WT for testing \n",
    "trialWT = data[data['scenario']=='WT']\n",
    "\n",
    "#Trial W and WT after making sure both training and testing sets have common subjects \n",
    "trialW_reduced, trialWT_reduced = keep_subjects_common_across_train_test(trialW, trialWT)\n",
    "# print ('Number of subjects in training and test sets after reduction:', len(trialW_reduced['PID'].unique()), \\\n",
    "#            len(trialWT_reduced['PID'].unique()))\n",
    "\n",
    "cols_to_drop = ['PID', 'key', 'cohort', 'trial', 'scenario', 'video', 'stride_number', 'label']\n",
    "#Shuffling the training stride data\n",
    "trialW_reduced = shuffle(trialW_reduced, random_state = 0)\n",
    "trainX = trialW_reduced.drop(cols_to_drop, axis = 1)\n",
    "trainY = trialW_reduced[['PID', 'label']]\n",
    "print ('Training shape', trainX.shape, trainY.shape)\n",
    "\n",
    "#Shuffling the testing stride data \n",
    "trialWT_reduced = shuffle(trialWT_reduced, random_state = 0)\n",
    "testX = trialWT_reduced.drop(cols_to_drop, axis = 1)\n",
    "testY = trialWT_reduced[['PID', 'label']] #PID to compute person based metrics later \n",
    "print ('Testing shape', testX.shape, testY.shape)\n",
    "\n",
    "#Normalize according to z-score standardization\n",
    "norm_mean, norm_sd = normalize(trainX, 'z')\n",
    "trainX_norm = (trainX-norm_mean)/norm_sd\n",
    "testX_norm = (testX-norm_mean)/norm_sd\n",
    "\n",
    "#Total strides and imbalance of labels in the training and testing set\n",
    "#Training set \n",
    "print('Strides in training set: ', len(trialW_reduced))\n",
    "print ('HOA, MS and PD strides in training set:\\n', trialW_reduced['cohort'].value_counts())\n",
    "\n",
    "#Test Set\n",
    "print('\\nStrides in test set: ', len(trialWT_reduced)) \n",
    "print ('HOA, MS and PD strides in test set:\\n', trialWT_reduced['cohort'].value_counts())\n",
    "print ('Imbalance ratio (controls:MS:PD)= 1:X:Y\\n', trialWT_reduced['cohort'].value_counts()/trialWT_reduced['cohort'].value_counts()['HOA'])\n",
    "\n",
    "framework = 'WtoWT' #Defining the task generalization framework of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_models = ['random_forest', 'adaboost', 'kernel_svm', 'gbm', 'xgboost', 'knn', 'decision_tree',  'linear_svm', \n",
    "             'logistic_regression', 'mlp']\n",
    "metrics = run_ml_models(ml_models, X, Y, framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
