{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gait Video Study \n",
    "### Validating the estimates 3D poses via CoP computed on the treadmill \n",
    "### This code makes (and saves as csv) the dataframe for each video containing their relevant frames, corresponding feet coordinates for these frames and their relative treadmill extracted CoP values \n",
    "\n",
    "We will do qualitative and quantitative validation for CoP. \n",
    "* First, we need to align the treadmill GaitCycles.csv file to the video time. This will help us align the corresponding video frames to the gait events i.e. HSR/HSL/TOR and TOL to be specific. \n",
    "* Once we know for each video, which frame numbers correspond to heel strikes and toe offs, we compute the sequence of frames that are in single support left phase, frames that are in single support right phase and similarly in double support phase. Thus, each frame of a video is labelled to be in SSR, SSL or DS phase. \n",
    "* Now, for each frame in DS phase, use the computed real world x, y coordinates for big toe, small toe and heel to make 2 triangular regions for both left and right feet, since in this phase, both feet are on ground and thus impact the center of pressure. Now, plot the corresponding actual COPX, COPY coordinate (as a red dot) for this particular frame. If this red dot lies in the shaded region of computed CoP drawn, we are good to claim that actual CoP lies in the approximate computed CoP region. Similarly, for each frame in the SSL phase, since left foot rests on ground for this phase, the CoP must be determined using the left foot, and hence use the computed x, y coordinates of the left big toe, small toe and heel to draw a shaded triangular region spanned by CoP for this frame, and draw the red dot for the actual x, y of CoP for this frame and if it lies within the shaded region, we are good to claim that actual and computed CoP region match. Now, for each frame in SSR phase, the shaded CoP region must be determined using the right feet's big/small toe and heel's x, y coordinates and if the actual CoP's x, y is bounded in this shaded computed CoP's region, we are good. \n",
    "\n",
    "* For qualitative validation, we plot these above mentioned regions for computed CoP and actual CoPs as markers in/out of that region for each video. If we do this for a complete stride, it should follow a butterfly pattern. And hence the inverted triangles and hexagons should occur in a butterfly pattern.\n",
    "* For quantitative validation, we will call it success (1) if the actual CoP is bounded by the computed CoP shaded region and failure (0) otherwise for every frame of every video for every trial and cohort. \n",
    "* Further, for more precise quantitative validation, we can find the lateral, anterior-posterior and euclidean distance of the actual (COP_x, COP_y) with the centroid of our region drawn. This gives us a numerical value quantifying the error in the true and predicted CoP. This step can especially be done for only wrongly predicted values, to further check what is the measure of wrongly predicted values. Further, we may check that we may have error most in the lateral direction or most in the AP direction or eucliean only. \n",
    "* Based on the statistics of these success and failure counts, we can quantify the performance of our marker estimation framework using CoP validation. \n",
    "* Further, we can try to correlate/have a look at the distribution pattern to relate the correctness of CoP (either quantified using binary scores or using the numerical scores) with the confidence scores predicted by the OpenPose algorithm. Now, since we are only using toes and heel coordinates to draw the CoM trajectory/region, we should only use the confidence scores for heel and toes for this correlation. To be precise, we can average the confidence scores of left feet's heel and 2 toes to get the aggregated confidence score for frames in left single support, similarly, we can average the confidence scores of right feet's heel and 2 toes to get the aggregated confidence score for frames in right single support, and average the confidence scores of both the left and right feet's heel and toes to get the aggregated confidence scores for frames in double support. Now this correlation/relation between the confidence score for each frame and it's coorsponding correctness of CoP metric can be either done on a frame by frame basis. Or rather we can aggreagte all frames over a stride and do the relationship analysis on a stride by stride basis based on some aggregatd confidence scores of stride with some aggregated correctness of CoP score over each stride. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import imports \n",
    "reload(imports)\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_file = pd.read_csv('C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\labels.csv', index_col = 0)\n",
    "# pd.DataFrame(labels_file.video.unique(), columns = ['video']).to_csv(cop_path+'treamill_video_cop_sync.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder for CoP validation\n",
    "cop_path = 'C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\CoPvalidation\\\\'\n",
    "#Subfolder containing the sync files between frame numbers and treadmill identified events for each video\n",
    "treadmill_video_sync_files = cop_path + 'CoP_treadmill_video_sync\\\\'\n",
    "#Path to store the new dataframes to be created for CoP validation \n",
    "path_viz_dataframes = cop_path + 'CoP_dataframes_for_viz\\\\'\n",
    "#Path to store the new dataframes to be created for CoP validation via non-hip height normalized frames \n",
    "path_viz_dataframes_non_normalized = path_viz_dataframes + 'non_hip_height_normalized\\\\'\n",
    "#Path to log file corresponding to the sync files between frame numbers and treadmill identified events for each video\n",
    "sync_log_file = cop_path + 'treamill_video_cop_sync.csv'\n",
    "\n",
    "#Path for reading the frame coordinates and OpenPose confidence scores from (toes and heel in particular)\n",
    "frame_path = cop_path + '..\\\\GaitVideoData\\\\video\\\\multi_view_merged_data\\\\' \n",
    "#Path for the RAWDATA.csv containing the COP values extracted by the treadmill wrt the time of the walking trial \n",
    "cop_treadmill_path = cop_path + '..\\\\GaitCSVData\\\\csv\\\\'\n",
    "\n",
    "#Configuration for which to run the code for \n",
    "cohorts = ['\\\\HOA', '\\\\MS', '\\\\PD', '\\\\ExtraHOA']\n",
    "trials = ['\\\\beam_walking', '\\\\walking']\n",
    "\n",
    "# for every GaitCycle file, a sequence of walk will always start with a heel strike on the right foot.\n",
    "# Thus the order of the Gait event points would be HSR, TOL, MidSSR, HSL, TOR and MidSSL.\n",
    "gait_type = np.array(['HSR', 'TOL', 'MidSSR', 'HSL', 'TOR', 'MidSSL'])\n",
    "\n",
    "trial_dict = {'BW': 'beam_walking', 'W': 'walking'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Need to run only once to create the new sync file \n",
    "# #Reading the log of the treadmill and video syncs \n",
    "# sync_log = pd.read_csv(sync_log_file, index_col = 0)\n",
    "# #Setting the new scenario column for marking the video as belonging to one of the W/WT/VBW/VBWT trials \n",
    "# sync_log.set_index('video', inplace=True)\n",
    "# sync_log['scenario'] = labels_file.groupby('video').first()['scenario']\n",
    "# sync_log.reset_index(inplace=True)\n",
    "# print ('Total video files: ', sync_log.shape[0])\n",
    "# #Saving the new sync file with scenario marked \n",
    "# sync_log.to_csv(sync_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid strides in the gait_cycles.csv file \n",
    "def get_cycle(dataframe):\n",
    "    stride_start = min(dataframe.loc[dataframe.EventType == 'HSR'].index)\n",
    "    stride_end = max(dataframe.loc[dataframe.EventType == 'MidSSL'].index)   \n",
    "    return dataframe.loc[stride_start:stride_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the indexing for the cropped dataframe \n",
    "def change_index(dataframe):\n",
    "    dataframe.index = range(len(dataframe))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the valid index in order: HSR-TOL-MidSSR-HSL-TOR-MidSSL\n",
    "def set_complete(data_frame):\n",
    "    # input is the Dataframe includes ONLY valid points \n",
    "    # get all the index of HSR since it starts with heal strike left\n",
    "    # if the length of last gait cycle contain HSR does not equals to 6, then ignore it\n",
    "    \n",
    "    HSR = data_frame.loc[data_frame.EventType == 'HSR'].index\n",
    "    last_idx = HSR[-1]\n",
    "    last_all_idx = data_frame.index[-1]\n",
    "    # if the last gait cycles contains HSR is not a valid gait cycle, then we should consider the last second HSR instead.\n",
    "    if((last_all_idx-last_idx) < 5):\n",
    "        HSR = HSR[0:-1] \n",
    "    else:\n",
    "        HSR = HSR\n",
    "    \n",
    "    # get all the valid index in order: HSR-TOL-MidSSR-HSL-TOR-MidSSL\n",
    "    valid = []\n",
    "    for idx_HSR in HSR:\n",
    "        if (((idx_HSR + 1) in data_frame.index) & ((idx_HSR + 2) in data_frame.index) &\n",
    "            ((idx_HSR + 3) in data_frame.index) & ((idx_HSR + 4) in data_frame.index) & \n",
    "            ((idx_HSR + 5) in data_frame.index)):\n",
    "            # the valid index exist in the dataframe.\n",
    "            if((data_frame.loc[idx_HSR + 1].EventType == 'TOL') & (data_frame.loc[idx_HSR + 2].EventType == 'MidSSR') & \n",
    "               (data_frame.loc[idx_HSR + 3].EventType == 'HSL') & (data_frame.loc[idx_HSR + 4].EventType == 'TOR') & \n",
    "               (data_frame.loc[idx_HSR + 5].EventType == 'MidSSL')):\n",
    "                valid.extend(range(idx_HSR, idx_HSR+6))\n",
    "    #returns the list of valid indices which form complete strides \n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the files to delete missing and invalid data \n",
    "def cleaning(gaitcycles_dataframe):         \n",
    "    #Reducing to complete strides data \n",
    "    #Making sure we start at the HSR and end at the MidSSL\n",
    "    gaitcycles_dataframe = get_cycle(gaitcycles_dataframe)\n",
    "    #Retaining only complete six even strides \n",
    "    indices_complete = set_complete(gaitcycles_dataframe)\n",
    "    gaitcycles_dataframe = gaitcycles_dataframe.loc[indices_complete]\n",
    "\n",
    "    #Resetting the index \n",
    "    gaitcycles_dataframe = change_index(gaitcycles_dataframe)\n",
    "    #Returning indices to identify consequetive and non-consequetive strides \n",
    "    return indices_complete, gaitcycles_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sync_files(video):\n",
    "    #Read the file for the current video syncing event types to frame numbers \n",
    "    video_csv = pd.read_excel(treadmill_video_sync_files + video + '.xlsx')\n",
    "    #Retaining only time for treadmill's CoP matching, event type for SS/DS group assignment \n",
    "    #and frame number for extracting body coordinates\n",
    "    video_csv = video_csv[['Time', 'EventType', 'frame_number']]\n",
    "    #Dropping the entries/events that could not be synced to their corresponding video frames \n",
    "    video_csv.dropna(inplace = True)\n",
    "    #Retaining the indices and corresponding rows of dataframe for 'complete' 6 event strides only\n",
    "    #Indices will help identify consequetive and non-consequetive strides \n",
    "    indices_retain, video_csv_retain = cleaning(video_csv)\n",
    "    #Converting frame number to ints \n",
    "    video_csv_retain['frame_number'] = video_csv_retain['frame_number'].astype(int)\n",
    "#     display(video_csv_retain.head(), video_csv.shape, video_csv_retain.shape)\n",
    "    return indices_retain, video_csv_retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_up_body_coordinates(viz_df, coordinate_path, coords_of_interest, coordinate_cols):\n",
    "    '''\n",
    "    Filling up the toe and heel coordinates in cm and confidence scores in the viz dataframe for the current video\n",
    "    '''\n",
    "    #Iterating through each frame number to read it's corresponding file for body coordinates and filling them up in the dataframe\n",
    "    for frame_number in viz_df.index:\n",
    "        try: #Since some frames from the video data may be missing \n",
    "            frame = pd.read_csv(coordinate_path+str(frame_number)+'.csv', index_col = 0)\n",
    "            #display(frame)\n",
    "            #For each frame, we are interested in only feet coordinates for drawing the CoM area\n",
    "            #Further, we use only x, y coordinates and confidence scores for this validation analysis \n",
    "            frame_coords_of_interest = frame.loc[coords_of_interest][['x', 'y', 'confidence']] \n",
    "            #Filling up the feet coordinates for a particular frame \n",
    "            viz_df.loc[frame_number, coordinate_cols] = frame_coords_of_interest.values.flatten()\n",
    "        except: \n",
    "            #If the particular frame was missing in video data, let the values be NaN for that missing video data frame  \n",
    "            pass\n",
    "#     display(viz_df.head())\n",
    "    return viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_up_treadmill_COP_values(viz_df, cop_path):\n",
    "    '''\n",
    "    Filling up the treadmill CoP values \n",
    "    '''\n",
    "    #For each trial/video, reading the corresponding RAWDATA.csv file containing the CoP_x, CoP_y values spaced at 0.002 seconds \n",
    "    cop_file = pd.read_csv(cop_path, header = 1)\n",
    "    #Retaining only time, COPX, COPY columns from the file\n",
    "    cop_file = cop_file[['Time', 'COPX', 'COPY']]\n",
    "    #     display(cop_file.head())\n",
    "\n",
    "    #Since the frame times are different than the 0.002 spacing of time in the RAWDATA.csv file, we find the \n",
    "    #closest time from the RAWDATA.csv file (since it's much more granular) to each frame's time in viz_df\n",
    "    #We use the fact that time is sorted in increasing order in the RAWDATA.csv file\n",
    "    cop_closest_times_left_bound = [cop_file['Time'][cop_file['Time']>(viz_df['Time'].iloc[i]-(1/60))].iloc[0] for i in range(len(viz_df))]\n",
    "    cop_closest_times_right_bound = [cop_file['Time'][cop_file['Time']<(viz_df['Time'].iloc[i]+(1/60))].iloc[-1] for i in range(len(viz_df))]\n",
    "    cop_file.set_index('Time', inplace = True)\n",
    "    #Assinging the COPX and COPY corresponding to the closest times in the RAWDATA.csv file to frame times \n",
    "    treadmill_COP_x = [cop_file.loc[i:j]['COPX'].mean() for i, j in zip(cop_closest_times_left_bound, cop_closest_times_right_bound)]\n",
    "    treadmill_COP_y = [cop_file.loc[i:j]['COPY'].mean() for i, j in zip(cop_closest_times_left_bound, cop_closest_times_right_bound)]\n",
    "    return treadmill_COP_x, treadmill_COP_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_up_support_types(viz_df):\n",
    "    '''\n",
    "    Filling up the support types\n",
    "    '''\n",
    "    #Marking the frame numbers of frames for HSR, TOL, HSL and TOR events \n",
    "    HSR_frames = viz_df[viz_df['EventType'] == 'HSR'].index\n",
    "    TOL_frames = viz_df[viz_df['EventType'] == 'TOL'].index\n",
    "    HSL_frames = viz_df[viz_df['EventType'] == 'HSL'].index\n",
    "    TOR_frames = viz_df[viz_df['EventType'] == 'TOR'].index\n",
    "\n",
    "    '''For our complete strides with sequence of events being: HSR-TOL-MidSSR-HSL-TOR-MidSSL-next stride's HSR, we compute the \n",
    "    initial double support as frames between (including) HSR and (not including) TOL, right single support as frames between (including) TOL \n",
    "    and (not including) HSL, terminal double support as frames between (including) HSL and (not including) TOR, left single support as \n",
    "    frame between (including) TOR and (not including) HSR. \n",
    "    The reason for including the left interval but not the right one, is because these events are typically treated as the \n",
    "    boundaries of the different states, so in the case of double support: HSR_time < time < TOL_time. \n",
    "    For single support, you can carry out the same process, where: TOL < time < HSL_time is right single support. \n",
    "    Now since when we compute the frame number, the frame no.s we assigned were one integer ahead round of float frame number \n",
    "    we got for each event. So according to HSR_time < time < TOL_time rule, if frame 231 is TOL_frame, then it should not belong to \n",
    "    double support but belong to right SS. \n",
    "    Hence for our case, double support frames are HSR_frame<=frames<TOL_frame, the right single support frames are TOL_frame<=frames<HSL_frame\n",
    "    and so on.\n",
    "    '''\n",
    "    #Initial double support\n",
    "    initial_double_support_indices = [viz_df.loc[HSR_frames[i]:TOL_frames[i]-1].index.values for i in range(len(HSR_frames))]\n",
    "    #-1 from the right limit of the interval to make sure we do not include frame of TOL to the initial double support frames list\n",
    "    initial_double_support_list = np.concatenate(initial_double_support_indices).ravel().tolist()\n",
    "\n",
    "    #Right single support \n",
    "    right_single_support_indices = [viz_df.loc[TOL_frames[i]:HSL_frames[i]-1].index.values for i in range(len(TOL_frames))]\n",
    "    #-1 from the right limit of the interval to make sure we do not include frame of HSL to the right single support frames list\n",
    "    right_single_support_list = np.concatenate(right_single_support_indices).ravel().tolist()\n",
    "\n",
    "    #Terminal double support\n",
    "    terminal_double_support_indices = [viz_df.loc[HSL_frames[i]:TOR_frames[i]-1].index.values for i in range(len(HSL_frames))]\n",
    "    terminal_double_support_list = np.concatenate(terminal_double_support_indices).ravel().tolist()\n",
    "\n",
    "    #Left single support\n",
    "    left_single_support_indices = [viz_df.loc[TOR_frames[i]:HSR_frames[i+1]-1].index.values for i in range(len(HSR_frames)-1)]\n",
    "    left_single_support_list = np.concatenate(left_single_support_indices).ravel().tolist()\n",
    "\n",
    "    return initial_double_support_indices, initial_double_support_list, right_single_support_indices, right_single_support_list, \\\n",
    "            terminal_double_support_indices, terminal_double_support_list, left_single_support_indices, left_single_support_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_non_consequetive_strides(indices_retain, video_csv_retain, viz_df):\n",
    "    #Handling non-consequetive strides \n",
    "    #Using the indices for the complete strides to infer inconsequtive strides' MidSSL frame number\n",
    "    non_consequetive_stride_MidSSLs = np.where(np.array(list(map(operator.sub, indices_retain[1:], indices_retain[:-1])))!=1)[0]\n",
    "    print ('No. of non-consequetive strides: ', len(non_consequetive_stride_MidSSLs))\n",
    "    #Looping through each non consequetive strides' MidSSL\n",
    "    for non_consequetive_stride_MidSSL in non_consequetive_stride_MidSSLs:\n",
    "        #Getting the frame nunber for TOR and HSR (since for non-consequetive strides, left single support from \n",
    "        #current stride's TOR-next strides' HSR is invalid!)\n",
    "        non_consequetive_stride_TOR = video_csv_retain.iloc[non_consequetive_stride_MidSSL-1].frame_number\n",
    "        non_consequetive_stride_HSR = video_csv_retain.iloc[non_consequetive_stride_MidSSL+1].frame_number\n",
    "        #Changing all the frames between TOR (including) and HSR (not including) support time to 'non_consequetive_stride' keyword\n",
    "        viz_df.loc[non_consequetive_stride_TOR:non_consequetive_stride_HSR-1]['support_type'] = 'non_consequetive_stride'\n",
    "    return viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_grouped_by_support_type(indices, coordinate_cols, viz_df):\n",
    "    #Starting frame numbers for initial double support \n",
    "#     display(viz_df)\n",
    "#     print (indices)\n",
    "    frame_start = [index_list[0] for index_list in indices]\n",
    "    #Ending frame numbers for initial double support\n",
    "    frame_end = [index_list[-1] for index_list in indices]\n",
    "    support_type = [viz_df.loc[index_list[0]].support_type for index_list in indices]\n",
    "    coordinate_cols_mean = [viz_df[coordinate_cols].loc[index_list].mean().values for index_list in indices]\n",
    "    treadmill_cop_mean = [viz_df[['treadmill_COP_x',  'treadmill_COP_y']].loc[index_list].mean().values for index_list in indices]\n",
    "\n",
    "    time_start = viz_df['Time'].loc[frame_start].values\n",
    "    time_end = viz_df['Time'].loc[frame_end].values\n",
    "\n",
    "    x = [sum(list([ [time_start[i]], [time_end[i]], [frame_start[i]], [frame_end[i]], [support_type[i]], list(coordinate_cols_mean[i]), list(treadmill_cop_mean[i])]), []) \\\n",
    "         for i in range(len(frame_start))]\n",
    "    return pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1): \n",
    "    x = np.array(list1) \n",
    "    return np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frame_and_support_group_dataframes(index, viz_df_column_names):\n",
    "    video = index['video']\n",
    "    cohort = index['cohort']\n",
    "    trial = trial_dict[index['trial']]\n",
    "    \n",
    "    #For each video, we will create a dataframe and a csv file corresponding to frame coordinates and treadmill CoP\n",
    "    viz_df = pd.DataFrame(columns = viz_df_column_names)\n",
    "    #Retaining the complete strides only from the synced treadmill six event times and video frames \n",
    "    indices_retain, video_csv_retain = clean_sync_files(video)\n",
    "    \n",
    "    #Filling up the event type, frame number and time columns\n",
    "    viz_df[['Time', 'EventType', 'frame_number']] = copy.deepcopy(video_csv_retain)\n",
    "    viz_df.set_index('frame_number', inplace=True)\n",
    "    #Listing all the frame numbers from the first HSR to the last MidSSL for viz_df \n",
    "#     print (viz_df.index.value_counts())\n",
    "    viz_df = viz_df.reindex(np.arange(min(viz_df.index), max(viz_df.index)+1))\n",
    "    #Filling up the time using interpolation (this will indeed follow that each frame is 1/30 seconds apart since our FPS=30)\n",
    "    viz_df.interpolate(method = 'index', inplace= True)\n",
    "    \n",
    "    #Filling up the toe and heel coordinates in cm and confidence scores in the viz dataframe for the current video\n",
    "    #To fill up the feet coordinates for each video, setting the path for body coordinate files \n",
    "    coordinate_path = frame_path+ cohort + '\\\\' + trial + '\\\\' + video + '\\\\' #hip_height_normalized\\\\'\n",
    "    if not os.path.exists(coordinate_path): #For ExtraHOA files \n",
    "        print ('ExtraHOA file!')\n",
    "        coordinate_path = frame_path+ 'ExtraHOA' + '\\\\' + trial + '\\\\' + video + '\\\\' #hip_height_normalized\\\\'\n",
    "    viz_df = fill_up_body_coordinates(viz_df, coordinate_path, coords_of_interest, coordinate_cols)\n",
    "#     print ('After filling the body coordinates: ')\n",
    "#     display (viz_df)\n",
    "    #Filling up the treadmill CoP values \n",
    "    #Note that the video extracted coordinates are in 'cm', but the treadmill's CoP are in 'm'\n",
    "    cop_path = cop_treadmill_path + cohort + '\\\\' + trial + '\\\\' + video  + '_RAWDATA.csv'\n",
    "    treadmill_COP_x, treadmill_COP_y = fill_up_treadmill_COP_values(viz_df, cop_path) \n",
    "    viz_df['treadmill_COP_x'] = treadmill_COP_x\n",
    "    viz_df['treadmill_COP_y'] = treadmill_COP_y\n",
    "        \n",
    "    #Filling up the support types \n",
    "    initial_double_support_indices, initial_double_support_list, right_single_support_indices, right_single_support_list, \\\n",
    "    terminal_double_support_indices, terminal_double_support_list, left_single_support_indices, left_single_support_list = fill_up_support_types(viz_df)\n",
    "\n",
    "    #For frame numbers corresponding to each support group, assigning the relative label to 'support_type' column in the viz dataframe\n",
    "    viz_df.loc[initial_double_support_list, 'support_type'] = 'initial DS'\n",
    "    viz_df.loc[right_single_support_list, 'support_type'] = 'right SS'\n",
    "    viz_df.loc[terminal_double_support_list, 'support_type'] = 'terminal DS'\n",
    "    viz_df.loc[left_single_support_list, 'support_type'] = 'left SS'\n",
    "    \n",
    "    #Since body coordinates are recorded in 'cm', but treadmill CoP in 'm', converting CoP values to 'cm'\n",
    "    viz_df['treadmill_COP_x'] = 100*viz_df['treadmill_COP_x']\n",
    "    viz_df['treadmill_COP_y'] = 100*viz_df['treadmill_COP_y']\n",
    "    \n",
    "    #Handling non consequetive strides \n",
    "    viz_df = handle_non_consequetive_strides(indices_retain, video_csv_retain, viz_df)\n",
    "    \n",
    "    #Saving 2 dataframes, one being frame wise for each video and other being group wise where we have 4 groups, namely initial/terminal\n",
    "    #double support and left/right single support per stride of the video \n",
    "    #Ideally, we should use the frame wise computed coordinates and CoPs for vizualization purposes and support group wise \n",
    "    #(basically, average all the frames per support group) computed coordinates and CoPs for validation purposes. \n",
    "    '''\n",
    "    You need the average COP and frame coordinates because you are trying to minimize noise. \n",
    "    You are trying to average across the few frames within each event to get an estimate of where the average COP position is.\n",
    "    '''\n",
    "    #New dataframe which contains coordinates and treadmill CoPs grouped by support type, i.e. each stride of the video has only 4 entries,\n",
    "    #for left/right SS and initial/terminal DS\n",
    "    #Columns for the new reduced dataframe are: frame_number_start, frame_number_end, time_start, time_end, support type, average of all\n",
    "    #coordinates, confidences and treadmill CoP coordinates\n",
    "\n",
    "    #Initial double support \n",
    "    initial_double_support_grouped_data = generate_data_grouped_by_support_type(initial_double_support_indices, coordinate_cols, viz_df)\n",
    "    #Right single support \n",
    "    right_single_support_grouped_data = generate_data_grouped_by_support_type(right_single_support_indices, coordinate_cols, viz_df)\n",
    "    #Terminal double support \n",
    "    terminal_double_support_grouped_data = generate_data_grouped_by_support_type(terminal_double_support_indices, coordinate_cols, viz_df)\n",
    "    #Left single support \n",
    "    left_single_support_grouped_data = generate_data_grouped_by_support_type(left_single_support_indices, coordinate_cols, viz_df)\n",
    "\n",
    "    #Concatenating all the four support type groups \n",
    "    viz_df_grouped_by_support_type = pd.concat((initial_double_support_grouped_data, right_single_support_grouped_data, terminal_double_support_grouped_data, \\\n",
    "               left_single_support_grouped_data), ignore_index=True)\n",
    "    viz_df_grouped_by_support_type.columns = viz_df_grouped_by_support_type_column_names\n",
    "    \n",
    "    #Sorting by starting frame number to arrange in correct strides order \n",
    "    viz_df_grouped_by_support_type = viz_df_grouped_by_support_type.sort_values(by = 'frame_number_start')\n",
    "    viz_df_grouped_by_support_type.reset_index(inplace = True)\n",
    "    viz_df_grouped_by_support_type.drop('index', axis = 1, inplace = True)\n",
    "    \n",
    "    viz_df.reset_index(inplace = True)\n",
    "    \n",
    "    #Saving both the frame wise and support group wise dataframes to .csvs\n",
    "    viz_df.to_csv(path_viz_dataframes_non_normalized+video+'.csv')\n",
    "    viz_df_grouped_by_support_type.to_csv(path_viz_dataframes_non_normalized+video+'_grouped_by_support_type.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video files:  107\n",
      "Total video files for which treadmill sync exists (sync time was present in the logs and treadmill software identified valid events):  102\n"
     ]
    }
   ],
   "source": [
    "sync_log = pd.read_csv(sync_log_file, index_col = 0)\n",
    "print ('Total video files: ', sync_log.shape[0])\n",
    "\n",
    "#Reducing only to video files for which treadmill sync exists \n",
    "#(sync time was present in the logs and treadmill software identified valid events)\n",
    "sync_log = sync_log[sync_log['Sync']=='Exists']\n",
    "print ('Total video files for which treadmill sync exists (sync time was present in the logs and treadmill software identified valid events): '\\\n",
    "       , sync_log.shape[0])\n",
    "\n",
    "'''\n",
    "Columns for the vizualization dataframe for each video \n",
    "Support type is left single support, right single support, double support or NaN for the left single support when \n",
    "the strides are not consequetive \n",
    "'HSR' - 'TOL': Initial double support \n",
    "'TOL' - MidSSR' - 'HSL': Right single support\n",
    "'HSL' - TOR': Terminal double support\n",
    "'TOR' - MidSSL' - Next 'HSR': Left single support (only when the strides are consequetive, else NaN)\n",
    "Extracted (x, y) for real world 3D coordinates and OpenPose confidence scores for the feet\n",
    "Treadmill's COP_x, COP_y for validation \n",
    "'''\n",
    "#We are only extracting feet coordinates to draw center of mass trajectory from the body coordinate files\n",
    "coords_of_interest = ['left toe 1', 'left toe 2', 'left heel', 'right toe 1', 'right toe 2', 'right heel']\n",
    "\n",
    "coordinate_cols = ['left toe 1-x', 'left toe 1-y', 'left toe 1-conf', 'left toe 2-x', \\\n",
    "                      'left toe 2-y', 'left toe 2-conf', 'left heel-x', 'left heel-y', 'left heel-conf', 'right toe 1-x', 'right toe 1-y',\\\n",
    "                       'right toe 1-conf', 'right toe 2-x', 'right toe 2-y', 'right toe 2-conf', 'right heel-x', 'right heel-y', \\\n",
    "                       'right heel-conf']\n",
    "viz_df_column_names = ['Time', 'EventType', 'frame_number', 'support_type'] + coordinate_cols + ['treadmill_COP_x',  'treadmill_COP_y']\n",
    "\n",
    "#Column names for dataframe grouped by support types \n",
    "viz_df_grouped_by_support_type_column_names = ['Time_start', 'Time_end', 'frame_number_start', 'frame_number_end', 'support_type'] + coordinate_cols + ['treadmill_COP_x',  'treadmill_COP_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GVS_212_T_T1\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_212_T_T1 completed in  492.29230546951294  seconds.\n",
      "Running GVS_213_T_T1\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_213_T_T1 completed in  1018.2159976959229  seconds.\n",
      "Running GVS_213_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_213_T_T2 completed in  804.9929299354553  seconds.\n",
      "Running GVS_214_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_214_T_T1 completed in  2851.298727273941  seconds.\n",
      "Running GVS_214_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_214_T_T2 completed in  1019.2483177185059  seconds.\n",
      "Running GVS_215_T_T1\n",
      "No. of non-consequetive strides:  2\n",
      "GVS_215_T_T1 completed in  1009.2990667819977  seconds.\n",
      "Running GVS_215_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_215_T_T2 completed in  1037.0890429019928  seconds.\n",
      "Running GVS_216_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_216_T_T1 completed in  1382.5507714748383  seconds.\n",
      "Running GVS_216_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_216_T_T2 completed in  1249.1275880336761  seconds.\n",
      "Running GVS_217_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_217_T_T1 completed in  1013.9128785133362  seconds.\n",
      "Running GVS_217_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_217_T_T2 completed in  991.2218616008759  seconds.\n",
      "Running GVS_218_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_218_T_T1 completed in  1006.1449627876282  seconds.\n",
      "Running GVS_218_T_T2\n",
      "No. of non-consequetive strides:  2\n",
      "GVS_218_T_T2 completed in  996.4184412956238  seconds.\n",
      "Running GVS_219_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_219_T_T1 completed in  906.3707003593445  seconds.\n",
      "Running GVS_219_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_219_T_T2 completed in  1002.4015998840332  seconds.\n",
      "Running GVS_212_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_212_W_T1 completed in  947.70596575737  seconds.\n",
      "Running GVS_212_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_212_W_T2 completed in  899.2122175693512  seconds.\n",
      "Running GVS_213_W_T1\n",
      "No. of non-consequetive strides:  2\n",
      "GVS_213_W_T1 completed in  931.7361319065094  seconds.\n",
      "Running GVS_213_W_T2\n",
      "No. of non-consequetive strides:  3\n",
      "GVS_213_W_T2 completed in  1011.0140879154205  seconds.\n",
      "Running GVS_214_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_214_W_T1 completed in  1009.6074576377869  seconds.\n",
      "Running GVS_214_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_214_W_T2 completed in  1041.9243664741516  seconds.\n",
      "Running GVS_215_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_215_W_T1 completed in  995.0910506248474  seconds.\n",
      "Running GVS_215_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_215_W_T2 completed in  1089.271276473999  seconds.\n",
      "Running GVS_216_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_216_W_T1 completed in  1070.5174496173859  seconds.\n",
      "Running GVS_216_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_216_W_T2 completed in  1032.6814925670624  seconds.\n",
      "Running GVS_217_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_217_W_T1 completed in  1115.1438617706299  seconds.\n",
      "Running GVS_217_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_217_W_T2 completed in  1052.2885403633118  seconds.\n",
      "Running GVS_218_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_218_W_T1 completed in  1025.1775970458984  seconds.\n",
      "Running GVS_218_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_218_W_T2 completed in  1104.4651265144348  seconds.\n",
      "Running GVS_219_W_T1\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_219_W_T1 completed in  1069.4781382083893  seconds.\n",
      "Running GVS_219_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_219_W_T2 completed in  1076.077773809433  seconds.\n",
      "Running GVS_310_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_310_T_T1 completed in  536.0492115020752  seconds.\n",
      "Running GVS_310_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_310_T_T2 completed in  675.7756974697113  seconds.\n",
      "Running GVS_311_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_311_T_T1 completed in  478.59088349342346  seconds.\n",
      "Running GVS_311_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_311_T_T2 completed in  950.9064409732819  seconds.\n",
      "Running GVS_312_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_312_T_T2 completed in  1014.3974494934082  seconds.\n",
      "Running GVS_318_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_318_T_T1 completed in  954.2224004268646  seconds.\n",
      "Running GVS_318_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_318_T_T2 completed in  982.188905954361  seconds.\n",
      "Running GVS_320_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_320_T_T1 completed in  842.2080161571503  seconds.\n",
      "Running GVS_320_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_320_T_T2 completed in  1468.8895723819733  seconds.\n",
      "Running GVS_321_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_321_T_T1 completed in  1194.2276816368103  seconds.\n",
      "Running GVS_321_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_321_T_T2 completed in  855.482744216919  seconds.\n",
      "Running GVS_323_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_323_T_T1 completed in  1848.507375240326  seconds.\n",
      "Running GVS_310_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_310_W_T1 completed in  1737.7661283016205  seconds.\n",
      "Running GVS_310_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_310_W_T2 completed in  2080.879865884781  seconds.\n",
      "Running GVS_311_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_311_W_T1 completed in  1246.8752522468567  seconds.\n",
      "Running GVS_311_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_311_W_T2 completed in  628.9319372177124  seconds.\n",
      "Running GVS_312_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_312_W_T2 completed in  1476.23650431633  seconds.\n",
      "Running GVS_313_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_313_W_T1 completed in  1169.9335570335388  seconds.\n",
      "Running GVS_313_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_313_W_T2 completed in  735.9482853412628  seconds.\n",
      "Running GVS_314_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_314_W_T1 completed in  1104.5661265850067  seconds.\n",
      "Running GVS_314_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_314_W_T2 completed in  1140.0337255001068  seconds.\n",
      "Running GVS_318_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_318_W_T1 completed in  445.5254137516022  seconds.\n",
      "Running GVS_318_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_318_W_T2 completed in  424.2915985584259  seconds.\n",
      "Running GVS_320_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_320_W_T1 completed in  1129.976016998291  seconds.\n",
      "Running GVS_320_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_320_W_T2 completed in  1121.2300279140472  seconds.\n",
      "Running GVS_321_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_321_W_T1 completed in  2655.6515679359436  seconds.\n",
      "Running GVS_321_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_321_W_T2 completed in  1076.0142068862915  seconds.\n",
      "Running GVS_322_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_322_W_T1 completed in  1069.423984527588  seconds.\n",
      "Running GVS_322_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_322_W_T2 completed in  1098.42538189888  seconds.\n",
      "Running GVS_323_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_323_W_T1 completed in  1016.1689500808716  seconds.\n",
      "Running GVS_323_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_323_W_T2 completed in  1128.1582765579224  seconds.\n",
      "Running GVS_403_T_T1\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_403_T_T1 completed in  253.72737336158752  seconds.\n",
      "Running GVS_403_T_T2\n",
      "No. of non-consequetive strides:  3\n",
      "GVS_403_T_T2 completed in  984.2820611000061  seconds.\n",
      "Running GVS_406_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_406_T_T1 completed in  837.3726494312286  seconds.\n",
      "Running GVS_406_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_406_T_T2 completed in  846.3340258598328  seconds.\n",
      "Running GVS_407_T_T1\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_407_T_T1 completed in  1071.1611256599426  seconds.\n",
      "Running GVS_408_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_408_T_T1 completed in  1017.1098341941833  seconds.\n",
      "Running GVS_408_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_408_T_T2 completed in  804.18554854393  seconds.\n",
      "Running GVS_409_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_409_T_T1 completed in  1105.1874148845673  seconds.\n",
      "Running GVS_409_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_409_T_T2 completed in  1074.9182024002075  seconds.\n",
      "Running GVS_410_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_410_T_T1 completed in  1144.6033127307892  seconds.\n",
      "Running GVS_410_T_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_410_T_T2 completed in  958.2499761581421  seconds.\n",
      "Running GVS_403_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_403_W_T2 completed in  917.3002419471741  seconds.\n",
      "Running GVS_404_W_T1\n",
      "No. of non-consequetive strides:  2\n",
      "GVS_404_W_T1 completed in  971.349221944809  seconds.\n",
      "Running GVS_404_W_T2\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_404_W_T2 completed in  1059.8954014778137  seconds.\n",
      "Running GVS_404_W_T3\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_404_W_T3 completed in  1057.084396123886  seconds.\n",
      "Running GVS_404_W_T4\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_404_W_T4 completed in  1070.0587780475616  seconds.\n",
      "Running GVS_405_W_T1\n",
      "No. of non-consequetive strides:  2\n",
      "GVS_405_W_T1 completed in  817.3882946968079  seconds.\n",
      "Running GVS_405_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_405_W_T2 completed in  388.08302783966064  seconds.\n",
      "Running GVS_405_W_T3\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_405_W_T3 completed in  560.2865226268768  seconds.\n",
      "Running GVS_405_W_T4\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_405_W_T4 completed in  981.4384491443634  seconds.\n",
      "Running GVS_406_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_406_W_T1 completed in  1048.4236171245575  seconds.\n",
      "Running GVS_406_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_406_W_T2 completed in  1051.8815495967865  seconds.\n",
      "Running GVS_407_W_T1\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_407_W_T1 completed in  1068.6007504463196  seconds.\n",
      "Running GVS_408_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_408_W_T1 completed in  1084.8964195251465  seconds.\n",
      "Running GVS_408_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_408_W_T2 completed in  1037.2483749389648  seconds.\n",
      "Running GVS_409_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_409_W_T1 completed in  1069.2824935913086  seconds.\n",
      "Running GVS_409_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_409_W_T2 completed in  3502.9374334812164  seconds.\n",
      "Running GVS_410_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_410_W_T1 completed in  1080.2075412273407  seconds.\n",
      "Running GVS_410_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_410_W_T2 completed in  1070.6607468128204  seconds.\n",
      "Running GVS_411_W_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_411_W_T1 completed in  1041.7718827724457  seconds.\n",
      "Running GVS_411_W_T2\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_411_W_T2 completed in  1161.3469252586365  seconds.\n",
      "Running GVS_411_W_T3\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_411_W_T3 completed in  1147.6342461109161  seconds.\n",
      "Running GVS_411_W_T4\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_411_W_T4 completed in  1165.4842822551727  seconds.\n",
      "Running GVS_102_W_T1\n",
      "ExtraHOA file!\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_102_W_T1 completed in  2416.9806056022644  seconds.\n",
      "Running GVS_112_W_T1\n",
      "ExtraHOA file!\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_112_W_T1 completed in  2233.114565372467  seconds.\n",
      "Running GVS_113_W_T1\n",
      "ExtraHOA file!\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_113_W_T1 completed in  1281.99777841568  seconds.\n",
      "Running GVS_115_W_T1\n",
      "ExtraHOA file!\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_115_W_T1 completed in  1506.7097806930542  seconds.\n",
      "Running GVS_123_W_T1\n",
      "ExtraHOA file!\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_123_W_T1 completed in  2368.788843393326  seconds.\n",
      "Running GVS_124_W_T1\n",
      "ExtraHOA file!\n",
      "No. of non-consequetive strides:  1\n",
      "GVS_124_W_T1 completed in  1914.769037246704  seconds.\n",
      "Running GVS_312_T_T1\n",
      "No. of non-consequetive strides:  0\n",
      "GVS_312_T_T1 completed in  1267.8897745609283  seconds.\n"
     ]
    }
   ],
   "source": [
    "#For each video with treadmill and video sync available \n",
    "for idx in range(len(sync_log)):\n",
    "    start_time = time.time()\n",
    "    index = sync_log.iloc[idx]\n",
    "    print ('Running', index['video'])\n",
    "    generate_frame_and_support_group_dataframes(index, viz_df_column_names)\n",
    "    print (index['video'], 'completed in ', time.time()-start_time, ' seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
