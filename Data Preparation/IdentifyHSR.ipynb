{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gait Video Study \n",
    "### Identifying frames with HSRs in each video for each cohort and trial to establish break points and also evaluate the corresponding HSR labelling via the ground truth available. Further, downsample with smoothing to define fixed shape of the input tensor for models. \n",
    "#### Remember to preserve the original count of frames in a single stride (before down sampling via smoothing) for each stride to add as an additional artificial feature later to add information about speed of the subject to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data'\n",
    "frame_path_merged = 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\multi_view_merged_data\\\\'\n",
    "\n",
    "#Configuration for which to run the code for \n",
    "cohorts = ['\\\\HOA', '\\\\MS', '\\\\PD', '\\\\ExtraHOA']\n",
    "trials = ['\\\\beam_walking', '\\\\walking']\n",
    "cameras = ['\\\\feet\\\\', '\\\\lower_body\\\\']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_212_T_T1_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_212_T_T2_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_213_T_T1_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_213_T_T2_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_214_T_T1_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_214_T_T2_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_215_T_T1_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_215_T_T2_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_216_T_T1_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_216_T_T2_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_217_T_T1_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_217_T_T2_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_218_T_T1_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_218_T_T2_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_219_T_T1_0_Trim\\\\HSRframes.txt'\n",
      "[]\n",
      "[Errno 2] No such file or directory: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\beam_walking\\\\feet\\\\InkedGVS_219_T_T2_0_Trim\\\\HSRframes.txt'\n",
      "['C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\walking\\\\feet\\\\InkedGVS_212_W_T1_0_Trim']\n",
      "[Errno 22] Invalid argument: 'C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\walking\\\\feet\\\\InkedGVS_212_W_T1_0_Trim\\\\HSRframes.txt'\n",
      "['C:\\\\Users\\\\purpl\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\frame_data\\\\HOA\\\\walking\\\\feet\\\\InkedGVS_212_W_T2_0_Trim']\n"
     ]
    }
   ],
   "source": [
    "#Saving the HSRframes.txt file to the hip_height_normalized\\\\ containing the final .csvs for analysis\n",
    "for cohort in cohorts:\n",
    "    for trial in trials:\n",
    "        merged_path = frame_path_merged+cohort+trial \n",
    "        if (os.path.exists(merged_path)):\n",
    "            videos = os.listdir(merged_path)\n",
    "#             print (len(videos))\n",
    "        for video in videos:\n",
    "            print (glob.glob(path+cohort+trial+'\\\\feet\\\\'+'Inked'+video+'_0_Trim'))\n",
    "            try:\n",
    "                if (not os.path.exists(merged_path+'\\\\'+video+'\\\\HSRframes.txt')):\n",
    "                    HSR_frames_file = path+cohort+trial+'\\\\feet\\\\'+'Inked'+video+'_0_Trim'+'\\\\HSRframes.txt'\n",
    "                    shutil.copy(HSR_frames_file, merged_path+'\\\\'+video+'\\\\hip_height_normalized\\\\') \n",
    "                    print ('HSR for', video, 'copied')\n",
    "                else:\n",
    "                    print ('HSR for', video, 'exists')\n",
    "            except Exception as e:\n",
    "                print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-50065119254f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mfront_video\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcohort\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\lower_body\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'Inked'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_1_Trim'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\hip_height_normalized\\\\*.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Dataframe to collect 40 features from all ~1500 frames of each video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "for cohort in cohorts:\n",
    "    for trial in trials:\n",
    "        merged_path = frame_path_merged+cohort+trial \n",
    "        if (os.path.exists(merged_path)):\n",
    "            videos = os.listdir(merged_path)\n",
    "#             print (len(videos))\n",
    "        for video in videos:\n",
    "            start_time = time.time()\n",
    "            front_video = path+cohort+trial+'\\\\lower_body\\\\'+'Inked'+video+'_1_Trim'\n",
    "            shutil.copy(file, merged_path+'\\\\'+video) \n",
    "            frames = glob.glob(merged_path+'\\\\'+video+'\\\\hip_height_normalized\\\\*.csv')\n",
    "            temp = pd.DataFrame(columns = labels) #Dataframe to collect 40 features from all ~1500 frames of each video\n",
    "            for frame in frames:\n",
    "#                     print (frame)\n",
    "                frame_csv = pd.read_csv(frame, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample with smoothing to define fixed shape input tensor for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mean with disjoint windows to downsample while smoothing \n",
    "#Make sure to preserve count of frames in a frame before smoothing to add as a feature \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
